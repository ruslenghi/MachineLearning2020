{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GROUP: Graphkrone\n",
    "# Members: Marcello Negri     19-945-450\n",
    "#          Riccardo Uslenghi  19-954-262\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "#from sklearn.experimental import enable_iterative_imputer  \n",
    "#from sklearn.impute import IterativeImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# validation_fractionfloat, default=0.1 FA LA CROSS VAL IN AUTOMATICO\n",
    "\n",
    "#out_file = open(\"solutions.txt\", \"w\")\n",
    "\n",
    "# loading dataset from 'train.csv', skipping first line (labels)\n",
    "data_trainX = np.loadtxt('train_features.csv', delimiter=\",\", skiprows=1)\n",
    "data_trainY = np.loadtxt('train_labels.csv', delimiter=\",\", skiprows=1)\n",
    "data_testX = np.loadtxt('test_features.csv', delimiter=\",\", skiprows=1)\n",
    "\n",
    "#trainXnn = np.loadtxt('./nn/train_nn_new.csv', delimiter=\",\", skiprows=1)\n",
    "#testXnn = np.loadtxt('./nn/test_nn_new.csv', delimiter=\",\", skiprows=1)\n",
    "\n",
    "\n",
    "pid = []\n",
    "for c in range(int(len(data_testX)/12)):\n",
    "    pid.append(int(data_testX[c*12,0]))\n",
    "\n",
    "trainY1 = data_trainY[:,1]\n",
    "trainY2 = data_trainY[:,2]\n",
    "trainY3 = data_trainY[:,3]\n",
    "trainY4 = data_trainY[:,4]\n",
    "trainY5 = data_trainY[:,5]\n",
    "trainY6 = data_trainY[:,6]\n",
    "trainY7 = data_trainY[:,7]\n",
    "trainY8 = data_trainY[:,8]\n",
    "trainY9 = data_trainY[:,9]\n",
    "trainY10 = data_trainY[:,10]\n",
    "trainY11 = data_trainY[:,11]\n",
    "trainY12 = data_trainY[:,12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocessing_zeros(data_train):\n",
    "    trainX = []\n",
    "    \n",
    "    for r in tqdm(range(int(len(data_train)/12))):\n",
    "        data_matrix = data_train[r*12:(r+1)*12,1:]\n",
    "        data_array = np.nanmean(data_matrix, axis=0)\n",
    "        trainX.append(data_array)\n",
    "     \n",
    "    trainX = np.array(trainX)\n",
    "    #trainX = (trainX - np.nanmean(trainX, axis=0))/np.nanstd(trainX, axis=0)\n",
    "    trainX[np.isnan(trainX)] = 0\n",
    "\n",
    "    return trainX\n",
    "\n",
    "def preprocessing(data_train):\n",
    "    trainX = []\n",
    "    \n",
    "    for r in range(int(len(data_train)/12)):\n",
    "        data_matrix = data_train[r*12:(r+1)*12,1:]\n",
    "        data_array = np.nanmean(data_matrix, axis=0)\n",
    "        trainX.append(data_array)\n",
    "    \n",
    "    trainX = np.array(trainX)\n",
    "    #trainX = (trainX - np.nanmean(trainX, axis=0))/np.nanstd(trainX, axis=0)\n",
    "    #trainX[np.isnan(trainX)] = 0\n",
    "\n",
    "    return trainX\n",
    "\n",
    "def delete(trainX, testX):\n",
    "    indices_columns = []\n",
    "    \n",
    "    for c in range(len(trainX[0])):\n",
    "        unique, count = np.unique(trainX[:,c], return_counts=True)\n",
    "        counter = dict(zip(unique, count))\n",
    "        if -999 in counter:\n",
    "            if counter[-999]/len(trainX[:,c])>0.70:\n",
    "                indices_columns.append(c)\n",
    "    \n",
    "    trainX = np.delete(trainX, indices_columns, axis=1)\n",
    "    testX = np.delete(testX, indices_columns, axis=1)\n",
    "    \n",
    "    return trainX, testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18995/18995 [00:01<00:00, 12413.75it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 1/7 [01:19<07:56, 79.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 2/7 [02:42<06:43, 80.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 3/7 [04:38<06:04, 91.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 4/7 [06:10<04:34, 91.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 5/7 [07:19<02:49, 84.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 6/7 [07:53<01:09, 69.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [09:45<00:00, 83.67s/it]\n"
     ]
    }
   ],
   "source": [
    "mlp_range = [4,18,21,24,25,30,34]\n",
    "\n",
    "trainX = preprocessing(data_trainX) # matrice compressa con nan\n",
    "attach = np.array(preprocessing_zeros(data_trainX)) # matrice compressa con nan=0\n",
    "\n",
    "for c in tqdm(mlp_range):\n",
    "    print(c)\n",
    "    M = np.c_[attach[:,:c],attach[:,(c+1):]] \n",
    "    v = trainX[:,c]\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for i in range(len(v)):\n",
    "        if np.isnan(v[i]) == False:\n",
    "            a.append(M[i])\n",
    "            b.append(v[i])\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    regressor1 = MLPRegressor(hidden_layer_sizes=(200,100 ), alpha=0.01, max_iter=200, n_iter_no_change=20)\n",
    "    #regressor1 = ElasticNet()\n",
    "    regressor1.fit(a, b)\n",
    "    \n",
    "    substitute = regressor1.predict(M)\n",
    "    \n",
    "    for r in range(len(M)):\n",
    "        if np.isnan(v[r]):\n",
    "            trainX[r,c]=substitute[r]\n",
    "            attach[r,c]=substitute[r]\n",
    "    for i in range(len(trainX)):\n",
    "        if np.isnan(trainX[i,c]):\n",
    "            print(\"male!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:16<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "elastic_range = [7,2,3,5,6,8,9,10,11,12,13,14,15,16,19,20,22,23,26,27,28,29,31,32,33,35]\n",
    "\n",
    "bad_col = trainX[:,17]\n",
    "avg = np.nanmean(bad_col)\n",
    "for i in range(len(bad_col)):\n",
    "    if np.isnan(bad_col[i]):\n",
    "        trainX[i,17]=avg\n",
    "\n",
    "for c in tqdm(elastic_range):\n",
    "    if c == 35: M = attach[:,:35]\n",
    "    else: M = np.c_[attach[:,:c],attach[:,(c+1):]] \n",
    "    v = trainX[:,c]\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for i in range(len(v)):\n",
    "        if np.isnan(v[i]) == False:\n",
    "            a.append(M[i])\n",
    "            b.append(v[i])\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    regressor2 = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "    regressor2.fit(a, b)\n",
    "    \n",
    "    substitute = regressor2.predict(M)\n",
    "    \n",
    "    for r in range(len(M)):\n",
    "        if np.isnan(v[r]):\n",
    "            trainX[r,c]=substitute[r]\n",
    "            attach[r,c]=substitute[r]\n",
    "    for i in range(len(trainX)):\n",
    "        if np.isnan(trainX[i,c]):\n",
    "            print(\"male!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.6748237072954548\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "count = 0\n",
    "\n",
    "for train_index, test_index in kf.split(trainX):\n",
    "    count += 1\n",
    "    \n",
    "    X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "    Y_train, Y_test = trainY11[train_index], trainY11[test_index]\n",
    "\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "    \n",
    "    classifier11 = SVC(C=1, probability=True, class_weight='balanced', verbose=True)\n",
    "    classifier11.fit(X_train, Y_train)\n",
    "    print(roc_auc_score(Y_test, classifier11.predict_proba(X_test)[:,1]))\n",
    "    if count > 0:\n",
    "        break\n",
    "#FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.6714220395947887\n"
     ]
    }
   ],
   "source": [
    "# KFOLD CLASS\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "count = 0\n",
    "\n",
    "for train_index, test_index in kf.split(trainX):\n",
    "    count += 1\n",
    "    \n",
    "    X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "    Y_train, Y_test = trainY9[train_index], trainY9[test_index]\n",
    "    \n",
    "    #X_train, Y_train = presepsis(X_train,Y_train)\n",
    "    #X_train, Y_train = down_sampling(X_train,Y_train)\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "    \n",
    "    #classifier1 = AdaBoostClassifier()\n",
    "    #classifier1 = MLPClassifier()\n",
    "    classifier1 = SVC(C=1, probability=True, class_weight='balanced', verbose=True)\n",
    "    classifier1.fit(X_train, Y_train)\n",
    "    print(roc_auc_score(Y_test, classifier1.predict_proba(X_test)[:,1]))\n",
    "    if count > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD SVR/ELASTIC-NET\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "scores = []\n",
    "count = 0\n",
    "\n",
    "#train_diff_X = preprocessing_hard(data_trainX)\n",
    "test_mean_X \n",
    "\n",
    "for train_index, test_index in kf.split(trainX):\n",
    "    count += 1\n",
    "    X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "    Y_train, Y_test = trainY15[train_index], trainY15[test_index]\n",
    "    \n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)    \n",
    "    \n",
    "    regressor = ElasticNet(alpha=0.01, l1_ratio=1)\n",
    "    #regressor = SVR()\n",
    "    regressor.fit(X_train, Y_train)\n",
    "    score_vero = 0.5 + 0.5 * np.maximum(0, r2_score(Y_test, regressor.predict(X_test)))\n",
    "    print('This is your score: ', score_vero)\n",
    "    if count > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.50000000e+00  3.40000000e+01  3.62465604e+01  2.69903865e+01\n",
      "   1.20000000e+01  1.49985343e+00  3.67500000e+01  8.56666667e+00\n",
      "   2.53333333e+01 -6.66666667e-01  1.70000000e+01  2.22405418e+02\n",
      "   4.60000000e+00  5.23333333e+00  5.00000000e-01  4.33333333e+01\n",
      "   3.59389260e+02  4.25000000e-01  1.43000000e+02  9.47815035e+01\n",
      "   1.20000000e+02  6.83333333e+01  1.80000000e+00  4.00000000e+00\n",
      "   5.02500000e+01  7.60000000e+00  4.79251808e+01  1.00000000e+02\n",
      "  -7.01856554e-01  1.12000000e+02  2.32000000e+01  7.70833333e+01\n",
      "  -3.26237874e-01 -1.33468921e+01  1.14500000e+02  7.37000000e+00]\n",
      " [            nan             nan  3.26523265e+01  4.11021348e+01\n",
      "              nan  1.70026761e+00  3.64414615e+01  1.39340314e+01\n",
      "   2.83208924e+01 -2.14537191e+01  1.84408791e+01  3.38438829e+02\n",
      "   2.99207002e+00  9.89689984e+00  5.87720474e-01  3.70495859e+01\n",
      "   5.59616388e+01             nan             nan  7.40145149e+01\n",
      "   1.37589301e+02             nan  1.96643207e+00  3.83525841e+00\n",
      "              nan             nan  7.65621572e+01  9.66695559e+01\n",
      "   2.66742133e-01  9.72108189e+01             nan  8.29030560e+01\n",
      "   1.24763026e+00  7.08628629e+00             nan  7.12254432e+00]\n",
      " [            nan             nan  3.26061321e+01  3.65859470e+01\n",
      "              nan  2.34053179e+00  3.69197194e+01  1.25533179e+01\n",
      "   2.44345514e+01 -2.23931719e+01  1.98695585e+01  3.50750654e+02\n",
      "   3.11967865e+00  1.29119728e+01  6.52331097e-01  3.76881240e+01\n",
      "   7.46672677e+01             nan             nan  7.12225523e+01\n",
      "   1.59727954e+02             nan  1.93845298e+00  3.99810068e+00\n",
      "              nan             nan  8.20489982e+01  9.66687604e+01\n",
      "  -3.18898825e-01  1.04189060e+02             nan  8.84749776e+01\n",
      "  -4.14291806e-01  8.59299001e+00             nan  7.10060459e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18995/18995 [00:01<00:00, 12986.99it/s]\n",
      "100%|██████████| 34/34 [00:09<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.50000000e+00  3.40000000e+01  3.75964272e+01  3.11757588e+01\n",
      "   1.20000000e+01  1.34452053e+00  3.67500000e+01  8.56666667e+00\n",
      "   2.53333333e+01 -6.66666667e-01  1.70000000e+01  2.38452783e+02\n",
      "   4.60000000e+00  5.23333333e+00  5.00000000e-01  4.33333333e+01\n",
      "   2.27690511e+02  4.25000000e-01  1.43000000e+02  9.44208118e+01\n",
      "   1.20000000e+02  6.83333333e+01  1.80000000e+00  4.00000000e+00\n",
      "   5.02500000e+01  7.60000000e+00  3.98264413e+01  1.00000000e+02\n",
      "   1.21832159e-01  1.12000000e+02  2.32000000e+01  7.70833333e+01\n",
      "   8.08218623e-01  2.08232113e+00  1.14500000e+02  7.37000000e+00]\n",
      " [ 6.50000000e+00  7.10000000e+01  3.22126482e+01  2.78000000e+01\n",
      "   1.20000000e+01  1.85722319e+00  3.60000000e+01  1.46000000e+01\n",
      "   2.92734570e+01  1.10630238e+00  1.80909091e+01  3.16697040e+02\n",
      "   2.50000000e+00  1.15000000e+01  8.20000000e-01  3.80516962e+01\n",
      "   2.00000000e+01  2.68215581e+00  2.07000000e+02  8.62377843e+01\n",
      "   1.52000000e+02  1.01727273e+02  1.50000000e+00  3.20000000e+00\n",
      "   8.32727273e+01  8.60000000e+00  6.80000000e+01  9.80000000e+01\n",
      "   4.08388997e-01  1.02936836e+02  4.21000000e+01  7.88181818e+01\n",
      "   1.30000000e+00  1.00000000e-02  1.32909091e+02  7.37768215e+00]\n",
      " [ 7.50000000e+00  6.80000000e+01  3.21361318e+01  2.09000000e+01\n",
      "   2.10000000e+01  2.59779474e+00  3.62500000e+01  1.25000000e+01\n",
      "   2.70000000e+01  2.65058196e+00  1.48333333e+01  3.13991874e+02\n",
      "   3.50000000e+00  1.25000000e+01  1.10000000e+00  4.18947605e+01\n",
      "   6.56196943e+01  1.47782354e+00  2.04000000e+02  8.23489508e+01\n",
      "   2.43000000e+02  8.18333333e+01  1.70000000e+00  3.60000000e+00\n",
      "   6.28333333e+01  9.00000000e+00  4.71585027e+01  9.65000000e+01\n",
      "   1.97858006e-01  1.01000000e+02  3.68000000e+01  1.09083333e+02\n",
      "   6.55644310e-01  1.02480301e+00  1.17000000e+02  7.37688569e+00]\n",
      " [ 6.50000000e+00  7.90000000e+01  3.18636364e+01  4.18883385e+01\n",
      "   2.20000000e+01  3.85500000e+00  3.68181818e+01  9.20000000e+00\n",
      "   2.92124282e+01 -1.40610481e-01  1.20000000e+01  2.58130594e+02\n",
      "   1.90000000e+00  1.96000000e+01  9.60000000e-01  4.40000000e+01\n",
      "   2.01756461e+02  4.00000000e-01  1.58000000e+02  9.80000000e+01\n",
      "   1.28625000e+02  8.34545455e+01  2.00000000e+00  3.96666667e+00\n",
      "   6.28181818e+01  3.46333333e+00  6.99390738e+01  9.88181818e+01\n",
      "  -3.39034780e-01  1.05160178e+02  2.73000000e+01  8.63636364e+01\n",
      "   2.39589154e-01  9.38061417e-01  1.41909091e+02  7.30000000e+00]\n",
      " [ 6.50000000e+00  7.60000000e+01  3.29254966e+01  2.85500000e+01\n",
      "   2.20000000e+01  1.69574203e+00  3.67500000e+01  1.07000000e+01\n",
      "   2.55000000e+01  1.50000000e+00  1.20909091e+01  2.22661698e+02\n",
      "   3.47619259e+00  7.75000000e+00  1.00000000e+00  4.45000000e+01\n",
      "   5.67280812e+01  5.00000000e-01  1.35000000e+02  9.82500000e+01\n",
      "   1.21750000e+02  6.90909091e+01  1.40000000e+00  3.90000000e+00\n",
      "   4.82272727e+01  6.67346929e+00  2.17415765e+01  9.85454545e+01\n",
      "  -1.31753727e-01  1.03500000e+02  3.03000000e+01  7.70909091e+01\n",
      "   5.59262941e-01 -5.29245880e+00  1.23000000e+02  7.39000000e+00]\n",
      " [ 6.50000000e+00  7.30000000e+01  1.90000000e+01  3.13000000e+01\n",
      "   1.80000000e+01  3.00500000e+00  3.70000000e+01  1.04000000e+01\n",
      "   2.30370868e+01 -1.16336680e+00  1.96250000e+01  1.61000000e+02\n",
      "   3.00000000e+00  1.03000000e+01  9.80000000e-01  4.05000000e+01\n",
      "   4.10000000e+01  8.00000000e-01  8.30000000e+01  9.24840442e+01\n",
      "   1.27166667e+02  6.98181818e+01  2.10000000e+00  4.47500000e+00\n",
      "   4.85000000e+01  3.12000000e+00  3.80000000e+01  9.91818182e+01\n",
      "   3.12929911e-01  1.07000000e+02  3.03000000e+01  6.70909091e+01\n",
      "   8.00000000e-01 -2.18664527e+01  1.32090909e+02  7.37500000e+00]\n",
      " [ 6.50000000e+00  5.10000000e+01  3.50394875e+01  4.52650455e+01\n",
      "   1.38758829e+01  2.36625193e+00  3.75000000e+01  1.78256802e+00\n",
      "   2.82942081e+01  1.92257333e-01  1.88888889e+01  2.23842952e+02\n",
      "   3.05208127e+00  7.66143442e+00  1.18131840e+00  3.91895370e+01\n",
      "   6.19128409e+01  7.71475636e-01  7.83880536e+01  8.55504549e+01\n",
      "   2.00500000e+02  7.05555556e+01  1.98388004e+00  4.02676844e+00\n",
      "   4.87142857e+01  6.26322491e+00  3.45281710e+01  9.66666667e+01\n",
      "  -1.61828580e-01  1.03776506e+02  1.12440497e+01  8.20000000e+01\n",
      "   4.69261870e-01  2.22945663e+00  1.17888889e+02  7.37695040e+00]\n",
      " [ 6.50000000e+00  6.00000000e+01  3.26452030e+01  4.72618098e+01\n",
      "   1.51054880e+01  1.69907017e+00  3.80000000e+01  1.85987679e+00\n",
      "   2.85131695e+01  8.41053675e-01  2.19090909e+01  2.51519469e+02\n",
      "   2.40000000e+00  5.68154906e+00  1.68724257e+00  3.86536852e+01\n",
      "   4.00851754e+01  4.40431483e-01  8.03975983e+01  8.59543667e+01\n",
      "   8.70000000e+01  1.08181818e+02  1.60000000e+00  3.60000000e+00\n",
      "   8.83636364e+01  6.69274971e+00  3.63020119e+01  9.49090909e+01\n",
      "  -2.61063284e-01  1.04058407e+02  1.27247034e+01  7.99090909e+01\n",
      "   3.52024680e-01  8.00000000e-02  1.39363636e+02  7.37733149e+00]\n",
      " [ 6.50000000e+00  6.90000000e+01  2.98047776e+01  8.60500000e+01\n",
      "   1.50000000e+01  2.41895954e+00  3.72500000e+01  1.22000000e+01\n",
      "   2.10000000e+01 -2.52651441e+00  2.25000000e+01  2.67950193e+02\n",
      "   3.10000000e+00  8.70000000e+00  5.00000000e-01  3.81686739e+01\n",
      "   1.70000000e+01  2.58405455e+00  1.82000000e+02  7.92332020e+01\n",
      "   1.09000000e+02  6.59090909e+01  1.20000000e+00  4.10000000e+00\n",
      "   6.52272727e+01  8.10000000e+00  6.40000000e+01  9.70909091e+01\n",
      "   4.37917820e-01  1.04000000e+02  3.40000000e+01  9.77272727e+01\n",
      "   8.00000000e-01  1.79791885e+01  9.09090909e+01  7.37773452e+00]\n",
      " [ 6.50000000e+00  3.60000000e+01  3.47364862e+01  3.12000000e+01\n",
      "   1.00000000e+01  1.80000000e+00  3.76666667e+01  1.04000000e+01\n",
      "   3.10000000e+01  7.50000000e+00  1.33636364e+01  3.24848525e+02\n",
      "   2.10000000e+00  8.70000000e+00  5.00000000e-01  4.02500000e+01\n",
      "   1.03718597e+02  4.16666667e-01  2.05000000e+02  9.54562634e+01\n",
      "   9.86666667e+01  8.59090909e+01  2.00000000e+00  3.10000000e+00\n",
      "   6.91818182e+01  8.50000000e+00  5.52868567e+01  1.00000000e+02\n",
      "   1.16242610e-01  1.05000000e+02  2.98000000e+01  1.06727273e+02\n",
      "   6.18588782e-01  1.08115439e+01  1.13090909e+02  7.51500000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainX = preprocessing(data_trainX) # matrice compressa con nan\n",
    "attach = np.array(preprocessing_zeros(data_trainX)) # matrice compressa con nan=0\n",
    "\n",
    "for c in tqdm(range(2,36)):\n",
    "    if c == 35: M = attach[:,:35]\n",
    "    else: M = np.c_[attach[:,:c],attach[:,(c+1):]] \n",
    "    v = trainX[:,c]\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for i in range(len(v)):\n",
    "        if np.isnan(v[i]) == False:\n",
    "            a.append(M[i])\n",
    "            b.append(v[i])\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    #regressor1 = MLPRegressor(hidden_layer_sizes=(200,100 ), alpha=0.01, max_iter=200, n_iter_no_change=20)\n",
    "    regressor1 = ElasticNet()\n",
    "    regressor1.fit(a, b)\n",
    "    \n",
    "    substitute = regressor1.predict(M)\n",
    "    \n",
    "    for r in range(len(M)):\n",
    "        if np.isnan(v[r]):\n",
    "            trainX[r,c]=substitute[r]\n",
    "            attach[r,c]=substitute[r]\n",
    "    for i in range(len(trainX)):\n",
    "        if np.isnan(trainX[i,c]):\n",
    "            print(\"male!!\")\n",
    "\n",
    "print(trainX[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier0 = SVC(probability=True, class_weight='balanced')\n",
    "classifier1 = SVC(probability=True, class_weight='balanced')\n",
    "classifier2 = SVC(probability=True, class_weight='balanced')\n",
    "classifier3 = SVC(probability=True, class_weight='balanced')\n",
    "classifier4 = SVC(probability=True, class_weight='balanced')\n",
    "classifier5 = SVC(probability=True, class_weight='balanced')\n",
    "classifier6 = SVC(probability=True, class_weight='balanced')\n",
    "classifier7 = SVC(probability=True, class_weight='balanced')\n",
    "classifier8 = SVC(probability=True, class_weight='balanced')\n",
    "classifier9 = SVC(probability=True, class_weight='balanced')\n",
    "classifier10 = SVC(probability=True, class_weight='balanced')\n",
    "classifier11 = SVC(probability=True, class_weight='balanced')\n",
    "\n",
    "regressor = MLPRegressor(hidden_layer_sizes=(100, ), \n",
    "                           activation='logistic',\n",
    "                           solver='adam',\n",
    "                           #max_iter=n_iter, \n",
    "                           #alpha=np.power(10., reg),\n",
    "                           verbose=10\n",
    "                           #tol=1e-4, \n",
    "                           #random_state=1,\n",
    "                           #learning_rate_init=.1\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:00<00:03,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 2, score = 0.14596275152555316, overfit = 0.15650150393005968\n",
      "column = 3, score = 0.01932831438868754, overfit = 0.0768216017574449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [00:00<00:03,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 5, score = 0.18814097782144723, overfit = 0.2573054599939968\n",
      "column = 6, score = 0.13796060659077736, overfit = 0.13819374679587937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [00:00<00:03,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 5/25 [00:00<00:03,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 7, score = 0.93072023483414, overfit = 0.9152983452169934\n",
      "column = 8, score = 0.46088229677914905, overfit = 0.5364512803137523"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:01<00:03,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "column = 9, score = 0.380381368197947, overfit = 0.40406454284545457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:01<00:02,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 10, score = 0.11842779994086239, overfit = 0.12303249999371624\n",
      "column = 11, score = 0.40939664092275296, overfit = 0.2853870511523853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 10/25 [00:01<00:02,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 12, score = 0.36404612442552753, overfit = 0.3976025155532199\n",
      "column = 13, score = 0.07590415549763252, overfit = 0.06512228916999252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 11/25 [00:01<00:02,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "column = 14, score = 0.4800904777954662, overfit = 0.4755747958402414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:02<00:02,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "column = 15, score = 0.1348610726517263, overfit = 0.1763194743509885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:02<00:01,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 16, score = 0.11856458225657118, overfit = 0.10608712270115106\n",
      "column = 19, score = 0.2752403009086376, overfit = 0.2774707657095883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:02<00:01,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 20, score = 0.04400246627930704, overfit = 0.04764152139103872\n",
      "column = 22, score = 0.11228937860679267, overfit = 0.08217428257197013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [00:03<00:01,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 23, score = 0.12704633751563466, overfit = 0.15730756591731088\n",
      "column = 26, score = 0.11816208233168668, overfit = 0.0791556338332956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [00:03<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 27, score = 0.09582019096156313, overfit = 0.10889903479449481\n",
      "column = 28, score = 0.23354020963777689, overfit = 0.9294827008855481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [00:03<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 29, score = 0.28928917779246144, overfit = 0.2992404449377556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:04<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 31, score = 0.1808557515947845, overfit = 0.17405764238583932\n",
      "column = 32, score = 0.1811609567245399, overfit = 0.2861270041607302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column = 33, score = 0.017874820335940234, overfit = 0.07928550300921211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "elastic_range = [2,3,5,6,7,8,9,10,11,12,13,14,15,16,19,20,22,23,26,27,28,29,31,32,33,35]\n",
    "\n",
    "for c in tqdm(elastic_range):\n",
    "    if c == 35: M = attach[:,:35]\n",
    "    else: M = np.c_[attach[:,:c],attach[:,(c+1):]] \n",
    "    M = np.c_[attach[:,:c],attach[:,(c+1):]]\n",
    "    v = trainX[:,c]\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for i in range(len(v)):\n",
    "        if np.isnan(v[i]) == False:\n",
    "            a.append(M[i])\n",
    "            b.append(v[i])\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "    count = 0\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "\n",
    "    for train_index, test_index in kf.split(a):\n",
    "        count += 1\n",
    "\n",
    "        X_train, X_test = a[train_index], a[test_index]\n",
    "        Y_train, Y_test = b[train_index], b[test_index]\n",
    "\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.5)\n",
    "        regressor.fit(X_train, Y_train)\n",
    "        \n",
    "        risultato = r2_score(Y_test, regressor.predict(X_test))\n",
    "        overfit = r2_score(Y_train, regressor.predict(X_train))\n",
    "\n",
    "        print('column = {0}, score = {1}, overfit = {2}'.format(c,risultato,overfit))\n",
    "        if count > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VA RIMESSO TESTX\n",
    "classifier0.fit(trainX, trainY1)\n",
    "M_0 = classifier0.predict_proba(trainX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1.fit(trainX, trainY1)\n",
    "M_1 = classifier1.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier2.fit(trainX, trainY2)\n",
    "M_2 = classifier2.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier3.fit(trainX, trainY3)\n",
    "M_3 = classifier3.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier4.fit(trainX, trainY4)\n",
    "M_4 = classifier4.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5.fit(trainX, trainY5)\n",
    "M_5 = classifier5.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier6.fit(trainX, trainY6)\n",
    "M_6 = classifier6.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier7.fit(trainX, trainY7)\n",
    "M_7 = classifier7.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier8.fit(trainX, trainY8)\n",
    "M_8 = classifier8.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier9.fit(trainX, trainY9)\n",
    "M_9 = classifier9.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier10.fit(trainX, trainY10)\n",
    "M_10 = classifier10.predict_proba(testX)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPSIS\n",
    "classifier11.fit(trainX_sepsis, trainY11)\n",
    "M_11 = classifier11.predict_proba(testX_sepsis)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_train = classifier11.predict_proba(trainX_sepsis)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2815.82433091\n",
      "Iteration 2, loss = 2467.37367954\n",
      "Iteration 3, loss = 2111.26558622\n",
      "Iteration 4, loss = 1760.07395876\n",
      "Iteration 5, loss = 1428.22039765\n",
      "Iteration 6, loss = 1127.48040290\n",
      "Iteration 7, loss = 866.50741490\n",
      "Iteration 8, loss = 649.61061872\n",
      "Iteration 9, loss = 476.45358049\n",
      "Iteration 10, loss = 343.68109495\n",
      "Iteration 11, loss = 245.66101177\n",
      "Iteration 12, loss = 176.05152511\n",
      "Iteration 13, loss = 128.29044072\n",
      "Iteration 14, loss = 96.61111084\n",
      "Iteration 15, loss = 76.21423071\n",
      "Iteration 16, loss = 63.39207169\n",
      "Iteration 17, loss = 55.49553245\n",
      "Iteration 18, loss = 50.69117735\n",
      "Iteration 19, loss = 47.74682866\n",
      "Iteration 20, loss = 45.92066468\n",
      "Iteration 21, loss = 44.74037671\n",
      "Iteration 22, loss = 43.93422123\n",
      "Iteration 23, loss = 43.35595614\n",
      "Iteration 24, loss = 42.90653528\n",
      "Iteration 25, loss = 42.54272207\n",
      "Iteration 26, loss = 42.24286667\n",
      "Iteration 27, loss = 41.97098815\n",
      "Iteration 28, loss = 41.72355924\n",
      "Iteration 29, loss = 41.49837414\n",
      "Iteration 30, loss = 41.27708349\n",
      "Iteration 31, loss = 41.05866753\n",
      "Iteration 32, loss = 40.84807637\n",
      "Iteration 33, loss = 40.63567985\n",
      "Iteration 34, loss = 40.42413682\n",
      "Iteration 35, loss = 40.20709988\n",
      "Iteration 36, loss = 39.98005028\n",
      "Iteration 37, loss = 39.74897016\n",
      "Iteration 38, loss = 39.49728399\n",
      "Iteration 39, loss = 39.23042394\n",
      "Iteration 40, loss = 38.94379801\n",
      "Iteration 41, loss = 38.63319994\n",
      "Iteration 42, loss = 38.29721220\n",
      "Iteration 43, loss = 37.92885649\n",
      "Iteration 44, loss = 37.52766312\n",
      "Iteration 45, loss = 37.09636381\n",
      "Iteration 46, loss = 36.62821718\n",
      "Iteration 47, loss = 36.12930230\n",
      "Iteration 48, loss = 35.61425892\n",
      "Iteration 49, loss = 35.05801034\n",
      "Iteration 50, loss = 34.47884801\n",
      "Iteration 51, loss = 33.89594703\n",
      "Iteration 52, loss = 33.28688647\n",
      "Iteration 53, loss = 32.67751153\n",
      "Iteration 54, loss = 32.06211079\n",
      "Iteration 55, loss = 31.44880658\n",
      "Iteration 56, loss = 30.83767154\n",
      "Iteration 57, loss = 30.23514629\n",
      "Iteration 58, loss = 29.64269588\n",
      "Iteration 59, loss = 29.07698204\n",
      "Iteration 60, loss = 28.52596025\n",
      "Iteration 61, loss = 27.99919237\n",
      "Iteration 62, loss = 27.49068089\n",
      "Iteration 63, loss = 27.00535318\n",
      "Iteration 64, loss = 26.55205674\n",
      "Iteration 65, loss = 26.11172100\n",
      "Iteration 66, loss = 25.70173890\n",
      "Iteration 67, loss = 25.29328050\n",
      "Iteration 68, loss = 24.90674137\n",
      "Iteration 69, loss = 24.54000006\n",
      "Iteration 70, loss = 24.18300841\n",
      "Iteration 71, loss = 23.85004041\n",
      "Iteration 72, loss = 23.54178713\n",
      "Iteration 73, loss = 23.25433990\n",
      "Iteration 74, loss = 22.98256359\n",
      "Iteration 75, loss = 22.72943986\n",
      "Iteration 76, loss = 22.48772091\n",
      "Iteration 77, loss = 22.26812936\n",
      "Iteration 78, loss = 22.06694793\n",
      "Iteration 79, loss = 21.87912224\n",
      "Iteration 80, loss = 21.70211763\n",
      "Iteration 81, loss = 21.54533501\n",
      "Iteration 82, loss = 21.39900880\n",
      "Iteration 83, loss = 21.26563820\n",
      "Iteration 84, loss = 21.13531229\n",
      "Iteration 85, loss = 21.02017836\n",
      "Iteration 86, loss = 20.91229690\n",
      "Iteration 87, loss = 20.81874538\n",
      "Iteration 88, loss = 20.73253947\n",
      "Iteration 89, loss = 20.64815917\n",
      "Iteration 90, loss = 20.57491582\n",
      "Iteration 91, loss = 20.49896845\n",
      "Iteration 92, loss = 20.43537047\n",
      "Iteration 93, loss = 20.37070737\n",
      "Iteration 94, loss = 20.31167501\n",
      "Iteration 95, loss = 20.25241471\n",
      "Iteration 96, loss = 20.20130919\n",
      "Iteration 97, loss = 20.14752804\n",
      "Iteration 98, loss = 20.09995136\n",
      "Iteration 99, loss = 20.06361332\n",
      "Iteration 100, loss = 20.02318355\n",
      "Iteration 101, loss = 19.96888848\n",
      "Iteration 102, loss = 19.93499985\n",
      "Iteration 103, loss = 19.89431258\n",
      "Iteration 104, loss = 19.85871925\n",
      "Iteration 105, loss = 19.82630872\n",
      "Iteration 106, loss = 19.78856094\n",
      "Iteration 107, loss = 19.75351691\n",
      "Iteration 108, loss = 19.73067758\n",
      "Iteration 109, loss = 19.70119709\n",
      "Iteration 110, loss = 19.66421542\n",
      "Iteration 111, loss = 19.63747851\n",
      "Iteration 112, loss = 19.61129097\n",
      "Iteration 113, loss = 19.58878036\n",
      "Iteration 114, loss = 19.56530316\n",
      "Iteration 115, loss = 19.53811777\n",
      "Iteration 116, loss = 19.51213119\n",
      "Iteration 117, loss = 19.48336319\n",
      "Iteration 118, loss = 19.46415898\n",
      "Iteration 119, loss = 19.44502750\n",
      "Iteration 120, loss = 19.42475397\n",
      "Iteration 121, loss = 19.40600217\n",
      "Iteration 122, loss = 19.38527336\n",
      "Iteration 123, loss = 19.35713356\n",
      "Iteration 124, loss = 19.34917256\n",
      "Iteration 125, loss = 19.32488544\n",
      "Iteration 126, loss = 19.30420076\n",
      "Iteration 127, loss = 19.28566230\n",
      "Iteration 128, loss = 19.26605541\n",
      "Iteration 129, loss = 19.24801207\n",
      "Iteration 130, loss = 19.22891868\n",
      "Iteration 131, loss = 19.22041511\n",
      "Iteration 132, loss = 19.19651847\n",
      "Iteration 133, loss = 19.18586955\n",
      "Iteration 134, loss = 19.17199162\n",
      "Iteration 135, loss = 19.15775877\n",
      "Iteration 136, loss = 19.13734836\n",
      "Iteration 137, loss = 19.12640918\n",
      "Iteration 138, loss = 19.10389840\n",
      "Iteration 139, loss = 19.09486225\n",
      "Iteration 140, loss = 19.07954707\n",
      "Iteration 141, loss = 19.06426849\n",
      "Iteration 142, loss = 19.05891557\n",
      "Iteration 143, loss = 19.04039058\n",
      "Iteration 144, loss = 19.03048581\n",
      "Iteration 145, loss = 19.01622300\n",
      "Iteration 146, loss = 18.99583957\n",
      "Iteration 147, loss = 18.99202060\n",
      "Iteration 148, loss = 18.97964346\n",
      "Iteration 149, loss = 18.96209297\n",
      "Iteration 150, loss = 18.95189002\n",
      "Iteration 151, loss = 18.94283847\n",
      "Iteration 152, loss = 18.93044988\n",
      "Iteration 153, loss = 18.91654780\n",
      "Iteration 154, loss = 18.90302402\n",
      "Iteration 155, loss = 18.89151115\n",
      "Iteration 156, loss = 18.88748374\n",
      "Iteration 157, loss = 18.87109500\n",
      "Iteration 158, loss = 18.85719308\n",
      "Iteration 159, loss = 18.84936116\n",
      "Iteration 160, loss = 18.83554114\n",
      "Iteration 161, loss = 18.82539294\n",
      "Iteration 162, loss = 18.81036683\n",
      "Iteration 163, loss = 18.80411099\n",
      "Iteration 164, loss = 18.80534132\n",
      "Iteration 165, loss = 18.79050424\n",
      "Iteration 166, loss = 18.77311019\n",
      "Iteration 167, loss = 18.77025059\n",
      "Iteration 168, loss = 18.75253285\n",
      "Iteration 169, loss = 18.73999748\n",
      "Iteration 170, loss = 18.72937738\n",
      "Iteration 171, loss = 18.72617681\n",
      "Iteration 172, loss = 18.71472217\n",
      "Iteration 173, loss = 18.71028710\n",
      "Iteration 174, loss = 18.70330834\n",
      "Iteration 175, loss = 18.69249548\n",
      "Iteration 176, loss = 18.67651780\n",
      "Iteration 177, loss = 18.66558305\n",
      "Iteration 178, loss = 18.66074699\n",
      "Iteration 179, loss = 18.65104470\n",
      "Iteration 180, loss = 18.64101619\n",
      "Iteration 181, loss = 18.63813544\n",
      "Iteration 182, loss = 18.63257814\n",
      "Iteration 183, loss = 18.62329919\n",
      "Iteration 184, loss = 18.60688675\n",
      "Iteration 185, loss = 18.59926200\n",
      "Iteration 186, loss = 18.59411463\n",
      "Iteration 187, loss = 18.57968356\n",
      "Iteration 188, loss = 18.57624258\n",
      "Iteration 189, loss = 18.56202980\n",
      "Iteration 190, loss = 18.56593010\n",
      "Iteration 191, loss = 18.55421993\n",
      "Iteration 192, loss = 18.54061480\n",
      "Iteration 193, loss = 18.54014952\n",
      "Iteration 194, loss = 18.52793548\n",
      "Iteration 195, loss = 18.52530809\n",
      "Iteration 196, loss = 18.51656692\n",
      "Iteration 197, loss = 18.50163313\n",
      "Iteration 198, loss = 18.49973222\n",
      "Iteration 199, loss = 18.48585157\n",
      "Iteration 200, loss = 18.47881831\n"
     ]
    }
   ],
   "source": [
    "# VITALS\n",
    "regressor.fit(trainX, trainY12)\n",
    "M_12 = regressor.predict(testX)\n",
    "\n",
    "# activation: 'logistic' -> Iteration 200, loss = 11.38286668\n",
    "# activation: 'relu' -> Iteration 200, loss = 10.59883991\n",
    "# preprocessing + logistic -> Iteration 200, loss = 18.61017290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "#Mzeros = np.zeros((12664,4))\n",
    "\n",
    "M = np.c_[ pid, M_1]\n",
    "M = np.c_[ M, M_2] \n",
    "M = np.c_[ M, M_3]\n",
    "M = np.c_[ M, M_4] \n",
    "M = np.c_[ M, M_5]\n",
    "M = np.c_[ M, M_6] \n",
    "M = np.c_[ M, M_7]\n",
    "M = np.c_[ M, M_8] \n",
    "M = np.c_[ M, M_9]\n",
    "M = np.c_[ M, M_10] \n",
    "M = np.c_[ M, M_11]\n",
    "M = np.c_[ M, M_12]\n",
    "\n",
    "'''\n",
    "pid_train = []\n",
    "\n",
    "for c in range(int(len(data_trainX)/12)):\n",
    "    pid_train.append(int(data_trainX[c*12,0]))\n",
    "'''\n",
    "\n",
    "df_predictions = pd.DataFrame(M, columns=['pid']+TESTS+['LABEL_Sepsis']+VITALS)\n",
    "df_predictions.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')\n",
    "\n",
    "# Printo la previsione sul train\n",
    "#pd.DataFrame(Mt, columns=['pid']+TESTS+['LABEL_Sepsis']+VITALS)\n",
    "#pd.DataFrame(M, columns=['pid']+TESTS+['LABEL_Sepsis']+VITALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MM = np.c_[M_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.168708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.028812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.029314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.089283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.096565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.068220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.099241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.029880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.043978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.080095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.032540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.026596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.086951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.051680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.115292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.046006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.029339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.105548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.028035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.127572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.031256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.091119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.140003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.127547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.056837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.032662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.108831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.193609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.145479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.030375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.174350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.025993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.059693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.056112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.028183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.059480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.030332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.025836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.027516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.027392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.060817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.142674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   0.076771\n",
       "1   0.039320\n",
       "2   0.062809\n",
       "3   0.090523\n",
       "4   0.032445\n",
       "5   0.168708\n",
       "6   0.028812\n",
       "7   0.029314\n",
       "8   0.089283\n",
       "9   0.096565\n",
       "10  0.050201\n",
       "11  0.055843\n",
       "12  0.068220\n",
       "13  0.099241\n",
       "14  0.029880\n",
       "15  0.043978\n",
       "16  0.029159\n",
       "17  0.080095\n",
       "18  0.032540\n",
       "19  0.026596\n",
       "20  0.086951\n",
       "21  0.051680\n",
       "22  0.115292\n",
       "23  0.046006\n",
       "24  0.029339\n",
       "25  0.105548\n",
       "26  0.028035\n",
       "27  0.127572\n",
       "28  0.031256\n",
       "29  0.091119\n",
       "30  0.140003\n",
       "31  0.127547\n",
       "32  0.056837\n",
       "33  0.032662\n",
       "34  0.108831\n",
       "35  0.193609\n",
       "36  0.145479\n",
       "37  0.030375\n",
       "38  0.174350\n",
       "39  0.025993\n",
       "40  0.059693\n",
       "41  0.056112\n",
       "42  0.028183\n",
       "43  0.059480\n",
       "44  0.030332\n",
       "45  0.025836\n",
       "46  0.027516\n",
       "47  0.027392\n",
       "48  0.060817\n",
       "49  0.142674"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(M_train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "def get_score(df_true, df_submission):\n",
    "    df_submission = df_submission.sort_values('pid')\n",
    "    df_true = df_true.sort_values('pid')\n",
    "    #task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])\n",
    "    #task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])\n",
    "    task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])\n",
    "    #score = np.mean([task1, task2, task3])\n",
    "    score = task3\n",
    "    #print(task1, task2, task3)\n",
    "    print(task3)\n",
    "    return score\n",
    "\n",
    "\n",
    "filename = 'sample.zip'\n",
    "df_submission = pd.read_csv(filename)\n",
    "\n",
    "# generate a baseline based on sample.zip\n",
    "df_true = pd.read_csv(filename)\n",
    "for label in TESTS + ['LABEL_Sepsis']:\n",
    "    # round classification labels\n",
    "    df_true[label] = np.around(df_true[label].values)\n",
    "\n",
    "print('Score of sample.zip with itself as groundtruth', get_score(df_true, df_submission))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mlp(dataset, hidden_layer_sizes, activation, solver, reg, noise):\n",
    "    np.random.seed(420)\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                               activation=activation,\n",
    "                               solver=solver,\n",
    "#                                max_iter=n_iter, \n",
    "                               alpha=np.power(10., reg),\n",
    "#                                verbose=10, \n",
    "#                                tol=1e-4, \n",
    "                               random_state=1,\n",
    "                               learning_rate_init=.1)\n",
    "\n",
    "\"\"\"hidden_layer_size=[(1,), (5, ), (50, ), (100, ), (1000, ),\n",
    "                                   (5, 5, ), (50, 50, ), (100, 100), \n",
    "                                   (50, 50, 50), (100, 100, 100)],\"\"\"\n",
    "\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.4)\n",
    "    \n",
    "classifier.fit(X_train, y_train)s\n",
    "print(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_2rows (data_train):\n",
    "    #overall_mean = np.nanmean(data_train[:,1:], axis=0)\n",
    "    trainX = []\n",
    "    \n",
    "    for r in range(int(len(data_train)/12)):\n",
    "        a = []\n",
    "        data_matrix_1 = data_train[r*12:int((r+0.5)*12),1:]\n",
    "        data_matrix_2 = data_train[int((r+0.5)*12):(r+1)*12,1:]\n",
    "        data_array_1 = np.nanmean(data_matrix_1, axis=0)\n",
    "        data_array_2 = np.nanmean(data_matrix_2, axis=0)\n",
    "        data_array_1[np.isnan(data_array_1)] = 0 # overall_mean[np.isnan(data_array)]\n",
    "        data_array_2[np.isnan(data_array_2)] = 0\n",
    "        a.append(data_array_1)\n",
    "        a.append(data_array_2)\n",
    "        trainX.append(np.ravel(a))\n",
    "    \n",
    "    return trainX\n",
    "\n",
    "def preprocessing (data_train):\n",
    "    #overall_mean = np.nanmean(data_train[:,1:], axis=0)\n",
    "    trainX = []\n",
    "    \n",
    "    for r in range(int(len(data_train)/12)):\n",
    "        data_matrix = data_train[r*12:(r+1)*12,1:]\n",
    "        data_array = np.nanmean(data_matrix, axis=0)\n",
    "        data_array[np.isnan(data_array)] = -999 # overall_mean[np.isnan(data_array)]\n",
    "        trainX.append(data_array)\n",
    "    \n",
    "    return trainX\n",
    "\n",
    "def preprocessing_easy (data_train):\n",
    "    #overall_mean = np.nanmean(data_train[:,1:], axis=0)\n",
    "    trainX = []\n",
    "    indices_columns = []\n",
    "    indices_rows = []\n",
    "    \n",
    "    for r in range(int(len(data_train)/12)):\n",
    "        data_matrix = data_train[r*12:(r+1)*12,1:]\n",
    "        data_array = np.nanmean(data_matrix, axis=0)\n",
    "        #data_array[np.isnan(data_array)] = -999 # overall_mean[np.isnan(data_array)]\n",
    "        trainX.append(data_array)\n",
    "        \n",
    "    trainX = np.array(trainX)\n",
    "    train_copy = np.array(trainX)\n",
    "    train_copy[np.isnan(train_copy)] = -999\n",
    "    \n",
    "    for c in range(len(trainX[0])):\n",
    "        unique, count = np.unique(train_copy[:,c], return_counts=True)\n",
    "        counter = dict(zip(unique, count))\n",
    "        if -999 in counter:\n",
    "            if counter[-999]/len(train_copy[:,c])>0.5:\n",
    "                indices_columns.append(c)\n",
    "    trainX = np.delete(trainX, indices_columns, axis=1)\n",
    "    \n",
    "    #print(pd.DataFrame(trainX))\n",
    "    \n",
    "    overall_mean = np.nanmean(trainX, axis=0)\n",
    "    train = []\n",
    "    for r in range(len(trainX)):\n",
    "        data_row = trainX[r,:]\n",
    "        data_row[np.isnan(data_row)] = 0 #overall_mean[np.isnan(data_row)]\n",
    "        train.append(data_row)\n",
    "    \n",
    "    #print(pd.DataFrame(train))\n",
    "    \n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.62338574e-01 -1.70642596e+00 -9.99000000e+02 -5.55684775e-01\n",
      " -3.58821385e-02 -1.05920327e+00  3.63105663e-01 -2.81148876e-01\n",
      "  7.49499010e-01 -6.94165878e-01 -5.11979019e-01  2.78323900e-01\n",
      " -2.15850856e-02 -6.56061723e-01 -3.90102865e-01 -1.03452538e+00\n",
      " -5.00153288e-01 -1.88862683e-01 -1.20593914e+00 -6.86480218e-02\n",
      "  1.07301695e+00  1.06502721e+00 -1.51879305e+00 -4.49558578e-01\n",
      " -4.16636694e-01 -8.20539698e-02]\n",
      "-------------------------->  500\n",
      "This is r:  13384\n",
      "[-1.60087537e-01  1.17080747e-01 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02]\n",
      "-------------------------->  500\n",
      "This is r:  17101\n",
      "[-1.60087537e-01 -1.22015751e+00 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02]\n",
      "-------------------------->  500\n",
      "This is r:  18417\n",
      "[-1.60087537e-01 -1.09859039e+00 -9.99000000e+02 -9.99000000e+02\n",
      "  3.37005691e-01 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02 -9.99000000e+02 -9.99000000e+02\n",
      " -9.99000000e+02 -9.99000000e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [22:33<00:00, 52.06s/it]\n",
      " 23%|██▎       | 6/26 [02:03<06:48, 20.45s/it]"
     ]
    }
   ],
   "source": [
    "def preprocessing(data_train):\n",
    "        #overall_mean = np.nanmean(data_train[:,1:], axis=0)\n",
    "    trainX = []\n",
    "    \n",
    "    for r in range(int(len(data_train)/12)):\n",
    "        data_matrix = data_train[r*12:(r+1)*12,1:]\n",
    "        data_array = np.nanmean(data_matrix, axis=0)\n",
    "        trainX.append(data_array)\n",
    " \n",
    "    trainX = (trainX - np.nanmean(trainX, axis=0))/np.nanstd(trainX, axis=0)\n",
    "    trainX[np.isnan(trainX)] = -999\n",
    "    \n",
    "    return trainX\n",
    "\n",
    "def delete(trainX, testX):\n",
    "    indices_columns = []\n",
    "    \n",
    "    for c in range(len(trainX[0])):\n",
    "        unique, count = np.unique(trainX[:,c], return_counts=True)\n",
    "        counter = dict(zip(unique, count))\n",
    "        if -999 in counter:\n",
    "            if counter[-999]/len(trainX[:,c])>0.70:\n",
    "                indices_columns.append(c)\n",
    "    \n",
    "    trainX = np.delete(trainX, indices_columns, axis=1)\n",
    "    testX = np.delete(testX, indices_columns, axis=1)\n",
    "    \n",
    "    return trainX, testX\n",
    "    \n",
    "    \n",
    "def preprocessing_nn (trainX):\n",
    "    \n",
    "    train_copy = trainX[:]\n",
    "    \n",
    "    def distance (matrix, r, s, alpha):\n",
    "        sum_, n_ = 0, 0\n",
    "        \n",
    "        for i in range(len(matrix[0])):\n",
    "            if matrix[r,i] != -999 and matrix[s,i] != -999:\n",
    "                sum_ += np.square(matrix[r,i]-matrix[s,i])\n",
    "                \n",
    "                n_ += 1\n",
    "        \n",
    "        if n_ >= 4:\n",
    "            return (n_**-alpha)*sum_\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    for c in tqdm(range(len(trainX[0]))):#len(trainX[0]):\n",
    "        for r in range(len(trainX)):\n",
    "            #if r%1000 == 0: print(r)\n",
    "            if trainX[r,c] == -999:\n",
    "                minimum = 1000\n",
    "                index_min = -1\n",
    "                count = 0\n",
    "                count_check = 0\n",
    "                s = r\n",
    "                while (count<100 and count_check<500):\n",
    "                    s += 1\n",
    "                    count_check += 1\n",
    "                    s = s%len(trainX)\n",
    "                    if trainX[s,c] != -999:\n",
    "                        dist = distance(trainX, r, s, 2)\n",
    "                        if dist:\n",
    "                            count += 1\n",
    "                            if dist < minimum:\n",
    "                                minimum = dist\n",
    "                                index_min = s\n",
    "                if count_check==500:\n",
    "                    print('--------------------------> ',count_check)\n",
    "                    print('This is r: ', r)\n",
    "                    print(trainX[r,:])\n",
    "                    train_copy[r,:] = np.zeros(len(trainX[0]))\n",
    "                else:\n",
    "                    train_copy[r,c] = trainX[index_min,c] #kick random??\n",
    "                    if trainX[index_min,c]==-999: print(\"WARNING: substituing -999! \", train_copy[r,c])\n",
    "\n",
    "    return train_copy\n",
    "\n",
    "def presepsis (trainX, trainY2):\n",
    "    trainX_sep = []\n",
    "    trainY2_sep = []\n",
    "    #unbalance = numpy.count_nonzero(a, axis=None)[source]/len(prep_trainX)\n",
    "    \n",
    "    for r in range(int(len(trainX))):\n",
    "        if trainY2[r] == 0:\n",
    "            trainX_sep.append(trainX[r])\n",
    "            trainY2_sep.append(trainY2[r])\n",
    "        else:\n",
    "            for s in range(10):\n",
    "                trainX_sep.append(trainX[r])\n",
    "                trainY2_sep.append(trainY2[r])\n",
    "    \n",
    "    return trainX_sep, trainY2_sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.99860896,  2.        ,  3.        ],\n",
       "       [ 4.        ,  3.49913153,  6.        ],\n",
       "       [10.        ,  4.99999933,  9.        ]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "imp_mean = IterativeImputer(random_state=0)\n",
    "imp_mean.fit([[np.nan, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
    "imp_mean.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2],[3,4]]\n",
    "np.ravel(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
